21 => 

Presentation on Speech-to-Text Models
Introduction
This presentation discusses the challenges faced in training previous models like Wav2Vec, HuBERT, and LLaMa for
speech-to-text conversions and text correction.

Wav2Vec
Wav2Vec, developed by Facebook AI for Automatic Speech Recognition (ASR), is trained in two phases: 
self-supervised learning and supervised fine-tuning. The model learns to understand and represent speech audio, 
similar to how word embeddings represent text. It does this by processing unlabeled audio data. 
The key advantage of Wav2Vec 2.0 is its ability to learn good speech representations from a large amount of 
unlabeled data. 
This allows it to achieve state-of-the-art results even with a small amount of labeled data. 
It uses Word Error Rate (WER) to measure the performance of ASR models.

Problem: Training Wav2Vec models can be computationally expensive, requiring powerful hardware and significant training time.
Impact: This can limit the accessibility of the model to those with limited resources.
Potential Solution: Potential solutions include using cloud-based resources or optimizing the model for efficiency.

HuBERT
    HuBERT operates by using an offline k-means clustering step to learn the structure of spoken input. 
    It predicts the correct cluster for masked audio segments, effectively learning the underlying patterns 
    in the speech data.

    Problem: HuBERT relies on k-means clustering to generate pseudo-labels for pre-training, and poor clustering 
    quality can lead to inaccurate labels.
    Impact: This can hinder model performance, especially when dealing with accented datasets.
    Potential Solution: Potential solutions include improving the clustering algorithm or using more diverse 
    training data.

LLaMa
Problem: LLaMa has specific input format requirements, and formatting and preprocessing custom data to 
ensure compatibility with the model's architecture can be challenging.
Impact: This can make it difficult to integrate the model with a speech-to-text system.
Potential Solution: Potential solutions include developing a preprocessing pipeline that automates 
the formatting process.

Whisper Model
Whisper is a Transformer-based encoder-decoder model used for ASR. 
It converts sequences from one domain (like speech audio) into sequences in another domain (like text). 
The WhisperProcessor pre-processes the audio inputs and post-processes the model outputs.

24 = >
This slide introduces OpenAI's Whisper, a model used for Automatic Speech Recognition (ASR). Here are the key points:

Whisper is an end-to-end speech recognition model developed by OpenAI, using an encoder-decoder Transformer
 architecture. It processes audio in 30-second chunks, converting them into log-Mel spectrograms for the 
 encoder, and the decoder predicts the corresponding text. Unlike models trained on specific datasets, 
 Whisper was trained on a large, diverse dataset, making it more robust across various datasets, even though 
 it may not outperform specialized models on specific benchmarks. Notably, Whisper is effective at learning 
 speech-to-text translation, especially for non-English languages, and outperforms the state-of-the-art on the 
 CoVoST2 to English translation task in a zero-shot setting.

25 = >
GPT-3:

Introduction: GPT-3, or Generative Pre-trained Transformer 3, is a cutting-edge language processing AI 
model developed by OpenAI.

Training: GPT-3 has been trained to understand and generate human-like text. This advanced training6
 enables it to perform a variety of tasks, including:

Language Translation: Converting text from one language to another.

Summarization: Condensing long pieces of text into shorter summaries.

Question-Answering: Providing accurate answers to questions based on the information it has been trained on.
Natural Language Understanding: GPT-3 excels in understanding natural language, the way humans naturally speak 
and write. This makes it highly effective in tasks involving human-computer interaction, 
content creation, and more.



26 = >
This slide presents a Python script that reads JSON data and a CSV file from a specific directory,
 converts the JSON data into a DataFrame, and then merges it with the CSV data. The merged data is 
 filtered for entries where the primary language is 'Tamil'. The 'audio/' part of the file names is 
 then removed. Finally, the processed data is saved as a new CSV file. This script is useful for data 
 preprocessing in a multilingual speech recognition context.


27 =>
This Python script uses the Hugging Face Transformers library and Gradio to create an interactive 
interface for an Automatic Speech Recognition (ASR) system. The ASR model, "Santhosh-kumar/ASR", 
transcribes audio input into text. The transcribed text is then corrected using OpenAI's GPT-3.5-turbo model. 
The corrected text is converted back into speech using the gTTS (Google Text-to-Speech) library and saved as a6
 WAV file. The Gradio interface allows users to input audio, and it displays the transcribed text, the corrected 
 text, and plays back the corrected speech.
